{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". \n",
    "# Performance Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multithreading & Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](multiprocessing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is a hard language to make multithreaded, mostly because of the **global interpreter lock**: Python is an interpreted language, and if the interpreter runs in only one thread, all the nice threads you're producing also run in only one thread.  \n",
    "While multi*processing* finds a way around that, even multithreading can still be of use. While multithreading can, in Python, not be **parallel**, it can still be **concurrent**. \n",
    "* *parallel* processes run truly at the same time - meaning that they must run sumultaneously on different CPU-cores\n",
    "* *concurrent* processes appear to be parallel to most of the system, even though the CPU may handle them one after another (either parallel or interlocked)\n",
    "\n",
    "While CPU-intense processes are only truly sped up when they are parallel (something where Python's multithreading doesn't help), tasks that have a bottleneck in network or disk access are helped greatly from cuncurrent execution already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multithreading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To a process, there's a lot of information for the computer:\n",
    "* State of the process (ready, running, inactive)\n",
    "* Program-counter (next commans)\n",
    "* CPU registers (cashed)\n",
    "* Scheduling information (priority, position)\n",
    "* Storage information\n",
    "* I/O-Status\n",
    "* ...\n",
    "\n",
    "When switching processes, all those information needs to be saved, such that the CPU can load another process and freeze this one. This produces a lot of overhead. \n",
    "\n",
    "**Threads** are lightweight processes, using *shared resources*:\n",
    "* Shared storage space\n",
    "* Shared program code\n",
    "* Shared (virtual) files.\n",
    "\n",
    "Modern operating systems use threads to let programs switch control, without all the overhead of having to save and load all the information.   \n",
    "**Advantages**:\n",
    "* Much faster creation and task switching\n",
    "* Efficient communication between threads (unlike processes)\n",
    "* Operating system doesn't schedule them, processes can implement their own scheduling\n",
    "\n",
    "**Disadvantages**:\n",
    "* Operating system doesn't schedule them, harder to synchronize than processes\n",
    "* Processes are better isolated\n",
    "* Crashing Thread = Crashing Program  \n",
    "* Python can't use them for parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.389281970004959 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "a = np.random.rand(1000000)\n",
    "\n",
    "def sorter():\n",
    "    np.sort(a)\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "for i in range(100):\n",
    "    sorter()\n",
    "\n",
    "print(time.perf_counter() - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6421769209991908 seconds\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "a = np.random.rand(1000000)\n",
    "\n",
    "def sorter():\n",
    "    np.sort(a)\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "threads = []\n",
    "for i in range(100):\n",
    "    t = threading.Thread(target=sorter)\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "print(time.perf_counter() - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recipe for creating threads:\n",
    "* Create a **child-thread** by calling the `threading.Thread`-constructor, specifying a **target function**\n",
    "* Save the reference to that thread somewhere, as you need it later(!!)\n",
    "* Start the child-thread. After you started it, your **main-thread** can continue doing other things that will done (seemingly) in parallel to the child.\n",
    "* Before your main-thread ends, `join` all threads, such that the main-thread waits until they are all done, too"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from urllib.request import urlopen, Request\n",
    "from time import time\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "ALLOWED_FILETYPES = ['.png', '.jpg', '.gif']\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "def get_links(client_id):\n",
    "   def get_img(json):\n",
    "     if os.path.splitext(json['link']) in ALLOWED_FILETYPES:\n",
    "        return [json['link']]\n",
    "     elif 'images' in json:\n",
    "        return [i['link'] for i in json['images']]\n",
    "     else:\n",
    "        return []\n",
    "    \n",
    "   headers = {'Authorization': 'Client-ID {}'.format(client_id)}\n",
    "   all_ims = []\n",
    "   for page in ['1', '2']:\n",
    "       req = Request('https://api.imgur.com/3/gallery/hot/viral/month', headers=headers, method='GET')\n",
    "       with urlopen(req) as resp:\n",
    "           data = json.loads(\"\\n\".join([i.decode('utf-8') for i in resp.readlines()]))\n",
    "       #pp.pprint(data)\n",
    "       all_ims = all_ims+[get_img(i) for i in data['data']]\n",
    "   return flatten(all_ims)\n",
    "\n",
    "\n",
    "def download_link(directory, link):\n",
    "   download_path = directory / os.path.basename(link)\n",
    "   with urlopen(link) as image, download_path.open('wb') as f:\n",
    "       for line in image.readlines():\n",
    "            f.write(line)\n",
    "\n",
    "        \n",
    "def setup_download_dir():\n",
    "   download_dir = Path('images')\n",
    "   if not download_dir.exists():\n",
    "       download_dir.mkdir()\n",
    "   return download_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 75.16112017631531s\n"
     ]
    }
   ],
   "source": [
    "from id import client_id\n",
    "\n",
    "ts = time()\n",
    "download_dir = setup_download_dir()\n",
    "links = [l for l in get_links(client_id) if os.path.splitext(l)[1] in ALLOWED_FILETYPES]\n",
    "for link in links:\n",
    "   download_link(download_dir, link)\n",
    "print('Took {}s'.format(time() - ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 28.135594606399536s\n"
     ]
    }
   ],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread\n",
    "from id import client_id\n",
    "\n",
    "class DownloadWorker(Thread):\n",
    "   def __init__(self, queue):\n",
    "       Thread.__init__(self)\n",
    "       self.queue = queue\n",
    "\n",
    "   def run(self):\n",
    "       while True:\n",
    "           # Get the work from the queue and expand the tuple\n",
    "           directory, link = self.queue.get()\n",
    "           download_link(directory, link)\n",
    "           self.queue.task_done()\n",
    "\n",
    "ts = time()\n",
    "download_dir = setup_download_dir()\n",
    "links = [l for l in get_links(client_id) if os.path.splitext(l)[1] in ALLOWED_FILETYPES]\n",
    "\n",
    "queue = Queue()                    # Create a queue to communicate with the worker threads\n",
    "for x in range(8):                 # Create 8 worker threads\n",
    "   worker = DownloadWorker(queue) \n",
    "   worker.daemon = True            # A daemon lets the main thread exit even though the workers are blocking\n",
    "   worker.start()                 \n",
    "\n",
    "for link in links:\n",
    "   queue.put((download_dir, link)) # Put the tasks into the queue as a tuple\n",
    "queue.join()                       # Causes the main thread to wait for the queue to finish processing all the tasks\n",
    "print('Took {}s'.format(time() - ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there is a class ``DownloadWorker``, which *is* a Python Thread. On every iteration of it's run, it calls ``self.queue.get()``, which fetches a URL from a thread-safe queue. Once the worker recieves such an item, it calls the ``download_link`` method that we used before. Then the worker must signal the queue that the task is done -- if not, ``queue.join()`` would block the main thread forever.  \n",
    "\n",
    "Note that while this method is concurrent, it is **not parallel** due to Python's GIL! - it is faster because the IO is the bottleneck in this task! The processor is mostly waiting, and can pick up working on a thread as soon as the network is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing \n",
    "If code is performing a CPU-heavy task, the execution time will probably be **slower**!  \n",
    "For such tasks, we need the ``multiprocessing`` module!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use multiple processing, what we generally do is to create a multiprocessing ``Pool``, which provides a method ``map``. This method is passed a list of URLs to the pool, which spawns individual processes - processes that can execute our download of the images *truly parallel*.  \n",
    "As mentioned above, the disadvantage of this method is that the entire memory of the script must be copied to every created subprocess, including all its overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 28.848686695098877s\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from multiprocessing.pool import Pool\n",
    "from id import client_id\n",
    "\n",
    "ts = time()\n",
    "download_dir = setup_download_dir()\n",
    "links = [l for l in get_links(client_id) if os.path.splitext(l)[1] in ALLOWED_FILETYPES]\n",
    "download = partial(download_link, download_dir)\n",
    "\n",
    "with Pool(8) as p:\n",
    "   p.map(download, links)\n",
    "print('Took {}s'.format(time() - ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.python.org/3/library/multiprocessing.html#using-a-pool-of-workers   \n",
    "https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing.pool   \n",
    "https://stackoverflow.com/questions/26520781/multiprocessing-pool-whats-the-difference-between-map-async-and-imap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map vs imap vs map_async"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`map` consumes your iterable by converting the iterable to a list (assuming it isn't a list already), breaking it into chunks, and sending those chunks to the worker processes in the `Pool`.  \n",
    "Breaking the iterable into chunks performs better than passing each item in the iterable between processes one item at a time - particularly if the iterable is large. However, turning the iterable into a list in order to chunk it can have a very high memory cost, since the entire list will need to be kept in memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 (Time elapsed: 5s)\n",
      "7 (Time elapsed: 5s)\n",
      "5 (Time elapsed: 5s)\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "def func(x):\n",
    "    time.sleep(x)\n",
    "    return x + 2\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    p = multiprocessing.Pool()\n",
    "    start = time.time()\n",
    "    for x in p.map(func, [1,5,3]):\n",
    "        print(\"{} (Time elapsed: {}s)\".format(x, int(time.time() - start)))\n",
    "        \n",
    "# I am expecting 3, 7, 5 appearing simultaenously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`imap` doesn't turn the iterable you give it into a list, nor does break it into chunks (by default). It will iterate over the iterable one element at a time, and send them each to a worker process.  \n",
    "This means you don't take the memory hit of converting the whole iterable to a list, but it also means the performance is slower for large iterables, because of the lack of chunking. This can be mitigated by passing a `chunksize` argument larger than default of 1, however."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other major difference between `imap/imap_unordered` and `map/map_async`, is that with imap/imap_unordered, you can start receiving results from workers as soon as they're ready, rather than having to wait for all of them to be finished.   \n",
    "With `map_async`, an `AsyncResult` is returned right away, but you can't actually retrieve results from that object until all of them have been processed, at which points it returns the same list that `map` does (map is actually implemented internally as `map_async(...).get()`). There's no way to get partial results; you either have the entire result, or nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 (Time elapsed: 1s)\n",
      "7 (Time elapsed: 5s)\n",
      "5 (Time elapsed: 5s)\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "def func(x):\n",
    "    time.sleep(x)\n",
    "    return x + 2\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    p = multiprocessing.Pool()\n",
    "    start = time.time()\n",
    "    for x in p.imap(func, [1,5,3]):\n",
    "        print(\"{} (Time elapsed: {}s)\".format(x, int(time.time() - start)))\n",
    "        \n",
    "# I am expecting 3, 7, 5 appearing once they're ready, but in the forced order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`imap` and `imap_unordered` both return iterables right away. With imap, the results will be `yielded` from the iterable as soon as they're ready, while still preserving the ordering of the input iterable. With `imap_unordered`, results will be `yielded` as soon as they're ready, regardless of the order of the input iterable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 (Time elapsed: 1s)\n",
      "5 (Time elapsed: 3s)\n",
      "7 (Time elapsed: 5s)\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "def func(x):\n",
    "    time.sleep(x)\n",
    "    return x + 2\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    p = multiprocessing.Pool()\n",
    "    start = time.time()\n",
    "    for x in p.imap_unordered(func, [1,5,3]):\n",
    "        print(\"{} (Time elapsed: {}s)\".format(x, int(time.time() - start)))\n",
    "        \n",
    "# I am expecting 3, 5, 7 appearing once they're ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primary reasons to use `imap/imap_unordered` over `map_async` are:\n",
    "* Your iterable is large enough that converting it to a list would cause you to run out of/use too much memory.\n",
    "* You want to be able to start processing the results before all of them are completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/35908987/multiprocessing-map-vs-map-async   \n",
    "https://stackoverflow.com/questions/11338044/python-multiprocessing-whats-the-difference-between-map-and-imap   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asynchronous programming: Async&Await\n",
    "\n",
    "Next to multiprocessing and multithreading for parallel processing, Python also provides the possibility for **asynchronous programming**, a different paradigm for parallel programming, mostly known from Javascript: [https://www.youtube.com/watch?v=3CmKIUmLmJo](https://www.youtube.com/watch?v=3CmKIUmLmJo)  \n",
    "\n",
    "For an example of that, it is referred to the full version of the source of this code and explanations at [https://www.toptal.com/python/beginners-guide-to-concurrency-and-parallelism-in-python](https://www.toptal.com/python/beginners-guide-to-concurrency-and-parallelism-in-python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numba\n",
    "Numba is an open source JIT compiler that translates a subset of Python and NumPy code into fast machine code. Numba-compiled numerical algorithms in Python can approach the speeds of C. Instead of having to re-write or even re-compile your code, you just have to add one decorator to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
       "       [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       "       [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
       "       [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n",
       "       [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n",
       "       [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n",
       "       [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\n",
       "       [70, 71, 72, 73, 74, 75, 76, 77, 78, 79],\n",
       "       [80, 81, 82, 83, 84, 85, 86, 87, 88, 89],\n",
       "       [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(100).reshape(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.  10.  11.  12.  13.  14.  15.  16.  17.  18.]\n",
      " [ 19.  20.  21.  22.  23.  24.  25.  26.  27.  28.]\n",
      " [ 29.  30.  31.  32.  33.  34.  35.  36.  37.  38.]\n",
      " [ 39.  40.  41.  42.  43.  44.  45.  46.  47.  48.]\n",
      " [ 49.  50.  51.  52.  53.  54.  55.  56.  57.  58.]\n",
      " [ 59.  60.  61.  62.  63.  64.  65.  66.  67.  68.]\n",
      " [ 69.  70.  71.  72.  73.  74.  75.  76.  77.  78.]\n",
      " [ 79.  80.  81.  82.  83.  84.  85.  86.  87.  88.]\n",
      " [ 89.  90.  91.  92.  93.  94.  95.  96.  97.  98.]\n",
      " [ 99. 100. 101. 102. 103. 104. 105. 106. 107. 108.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def go_slow(a):\n",
    "    trace = 0\n",
    "    for i in range(a.shape[0]):   \n",
    "        trace += np.tanh(a[i, i]) \n",
    "    return a + trace \n",
    "\n",
    "print(go_slow(np.arange(100).reshape(10, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "\n",
    "@jit(nopython=True) \n",
    "def go_fast(a):\n",
    "    trace = 0\n",
    "    for i in range(a.shape[0]):   \n",
    "        trace += np.tanh(a[i, i]) \n",
    "    return a + trace    \n",
    "\n",
    "go_fast(np.arange(100).reshape(10, 10)); #we need to compile the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = np.arange(10_000).reshape(100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 556 µs, sys: 262 µs, total: 818 µs\n",
      "Wall time: 706 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   99.,   100.,   101., ...,   196.,   197.,   198.],\n",
       "       [  199.,   200.,   201., ...,   296.,   297.,   298.],\n",
       "       [  299.,   300.,   301., ...,   396.,   397.,   398.],\n",
       "       ...,\n",
       "       [ 9799.,  9800.,  9801., ...,  9896.,  9897.,  9898.],\n",
       "       [ 9899.,  9900.,  9901., ...,  9996.,  9997.,  9998.],\n",
       "       [ 9999., 10000., 10001., ..., 10096., 10097., 10098.]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "go_slow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 163 µs, sys: 77 µs, total: 240 µs\n",
      "Wall time: 260 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   99.,   100.,   101., ...,   196.,   197.,   198.],\n",
       "       [  199.,   200.,   201., ...,   296.,   297.,   298.],\n",
       "       [  299.,   300.,   301., ...,   396.,   397.,   398.],\n",
       "       ...,\n",
       "       [ 9799.,  9800.,  9801., ...,  9896.,  9897.,  9898.],\n",
       "       [ 9899.,  9900.,  9901., ...,  9996.,  9997.,  9998.],\n",
       "       [ 9999., 10000., 10001., ..., 10096., 10097., 10098.]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "go_fast(x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<see the notebooks in the numba-directory>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cython\n",
    "One way to make your program run faster: compile to a faster language.\n",
    "[https://cython.org/](https://cython.org/)\n",
    "\n",
    "### About Cython\n",
    "\n",
    "Cython is an optimising static compiler for both the Python programming language and the extended Cython programming language (based on Pyrex). It makes writing C extensions for Python as easy as Python itself.\n",
    "\n",
    "Cython gives you the combined power of Python and C to let you\n",
    "\n",
    "* write Python code that calls back and forth from and to C or C++ code natively at any point.\n",
    "* easily tune readable Python code into plain C performance by adding static type declarations, also in Python syntax.\n",
    "* use combined source code level debugging to find bugs in your Python, Cython and C code.\n",
    "* interact efficiently with large data sets, e.g. using multi-dimensional NumPy arrays.\n",
    "* quickly build your applications within the large, mature and widely used CPython ecosystem.\n",
    "* integrate natively with existing code and data from legacy, low-level or high-performance libraries and applications.\n",
    "\n",
    "The Cython language is a superset of the Python language that additionally supports calling C functions and declaring C types on variables and class attributes. This allows the compiler to generate very efficient C code from Cython code. The C code is generated once and then compiles with all major C/C++ compilers in CPython 2.6, 2.7 (2.4+ with Cython 0.20.x) as well as 3.3 and all later versions. We regularly run integration tests against all supported CPython versions and their latest in-development branches to make sure that the generated code stays widely compatible and well adapted to each version. PyPy support is work in progress (on both sides) and is considered mostly usable since Cython 0.17. The latest PyPy version is always recommended here.\n",
    "\n",
    "All of this makes Cython the ideal language for wrapping external C libraries, embedding CPython into existing applications, and for fast C modules that speed up the execution of Python code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro \n",
    "\n",
    "[https://cython.readthedocs.io/en/latest/src/tutorial/cython_tutorial.html](https://cython.readthedocs.io/en/latest/src/tutorial/cython_tutorial.html) \n",
    "\n",
    "The fundamental nature of Cython can be summed up as follows: Cython is Python with C data types.\n",
    "\n",
    "Cython is Python: Almost any piece of Python code is also valid Cython code. (There are a few Limitations, but this approximation will serve for now.) The Cython compiler will convert it into C code which makes equivalent calls to the Python/C API.\n",
    "\n",
    "But Cython is much more than that, because parameters and variables can be declared to have C data types. Code which manipulates Python values and C values can be freely intermixed, with conversions occurring automatically wherever possible. Reference count maintenance and error checking of Python operations is also automatic, and the full power of Python’s exception handling facilities, including the try-except and try-finally statements, is available to you – even in the midst of manipulating C data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an introduction on how to use Cython, I recommend [reading the docs.](https://cython.readthedocs.io/en/latest/src/tutorial/cython_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<see the cython-directory>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fib_python(n):\n",
    "    \"\"\"Print the Fibonacci series up to n.\"\"\"\n",
    "    a, b = 0, 1\n",
    "    while b < n:\n",
    "        print(b, end=' ')\n",
    "        a, b = b, a + b\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 1597 2584 4181 6765 10946 17711 28657 46368 75025 121393 196418 317811 514229 832040 1346269 2178309 3524578 5702887 9227465 14930352 24157817 39088169 63245986 102334155 165580141 267914296 433494437 701408733 1134903170 1836311903 2971215073 4807526976 7778742049 \n",
      "CPU times: user 6.67 ms, sys: 5.61 ms, total: 12.3 ms\n",
      "Wall time: 7.16 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "fib_python(1e+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./cython')\n",
    "from fibonacci import fib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 1597 2584 4181 6765 10946 17711 28657 46368 75025 121393 196418 317811 514229 832040 1346269 2178309 3524578 5702887 9227465 14930352 24157817 39088169 63245986 102334155 165580141 267914296 433494437 701408733 1134903170 1836311903 2971215073 4807526976 7778742049 \n",
      "CPU times: user 5.26 ms, sys: 0 ns, total: 5.26 ms\n",
      "Wall time: 3.03 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "fib.fib(1e+10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Computing\n",
    "\n",
    "Another way to make it faster: make a computation graph & distribute. As we don't have time for this, I recommend looking into [Dask](https://dask.org/) if you're interested."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
